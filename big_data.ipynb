{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Aplicatie pentru recunoasterea genurilor muzicale",
   "id": "740f8320ecdd967e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Introducere - prezentarea setului de date si enuntarea obiectivelor",
   "id": "29e2dfa316538119"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In cadrul acestui proiect am utilizat setul de date disponibil pe platforma Kaggle:\n",
    "https://www.kaggle.com/datasets/andradaolteanu/gtzan-dataset-music-genre-classification\n",
    "\n",
    "Folosind acest dataset am ales sa construiesc o aplicatie de recunoastere a genurilor muzicale pe baza input-urilor audio furnizate.\n",
    "\n",
    "In cadrul dataset-ului regasim atat fisiere .wav cu 10 categorii muzicale si 100 de exemple in fiecare dintre acestea, cat si doua fisiere .csv cu datele importante deja extrase din fisierele audio. Totodata, regasim si distributia liniara a sunetelor pentru fiecare sample melodic, in format .png.\n",
    "\n",
    "Totusi, pentru a rezolva cerintele proiectului am utilizat fisierele .wav pentru a extrage exact datele de care aveam nevoie."
   ],
   "id": "52166d58bf8c3741"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Preprocesarea datelor",
   "id": "3359bb4137281c39"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Asa cum enuntam mai sus, am utilizat intreaga colectie de fisiere .wav puse la dispozitie de catre datasetul ales. Scopul este acela de a extrage feature-urile care ne vor fi de folos pentru antrenarea modelelor. Am realizat acest lucru folosind libraria Python _librosa_, care ajuta la analiza si procesarea semnalelor audio.",
   "id": "2a04f0fedf16f838"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T17:43:47.063477Z",
     "start_time": "2025-06-30T17:37:16.692149Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import librosa\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from librosa.feature.rhythm import tempo as rhythm_tempo\n",
    "\n",
    "base_path = \"data/Data/genres_original\"\n",
    "genres = os.listdir(base_path)\n",
    "features = []\n",
    "\n",
    "for genre in genres:\n",
    "    genre_path = os.path.join(base_path, genre)\n",
    "    for file in os.listdir(genre_path):\n",
    "        if file.endswith(\".wav\"):\n",
    "            path = os.path.join(genre_path, file)\n",
    "            try:\n",
    "                y, sr = librosa.load(path)\n",
    "                tempo_val = rhythm_tempo(y=y, sr=sr)[0] # variatia de ritm a piesei\n",
    "                spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr).mean() # inaltimea sunetului\n",
    "                zero_crossing_rate = librosa.feature.zero_crossing_rate(y).mean() # frecventa valorii 0 in evolutia semnalului - ajuta la identificarea zgomotelor percutante\n",
    "                rmse = librosa.feature.rms(y=y).mean() # Root Mean Square Energy - energia generala a semnalului\n",
    "                bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr).mean() # latimea benzii de frecventa - urmareste cat de mult variaza semnalul\n",
    "                chroma = librosa.feature.chroma_stft(y=y, sr=sr).mean() # distributia semnalului pe notele muzicale - reflecta tonalitatea\n",
    "                mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13) # Mel-Frequency Cepstral Coefficients - reprezentare compacta a sunetului, apropiata de perceptia umana\n",
    "                mfccs_mean = mfccs.mean(axis=1) # calculam media valorilor mfcc extrase\n",
    "                beat_strength = np.mean(librosa.onset.onset_strength(y=y, sr=sr)) # puterea batailor\n",
    "                tempogram = librosa.feature.tempogram(y=y, sr=sr)\n",
    "                tempo_variation = np.std(tempogram) # deviatia tempo-ului\n",
    "                onsets = librosa.onset.onset_detect(y=y, sr=sr)\n",
    "                onset_density = len(onsets) / librosa.get_duration(y=y, sr=sr) # densitatea evenimentelor sonore\n",
    "\n",
    "                feature = {\n",
    "                    'filename': file,\n",
    "                    'genre': genre,\n",
    "                    'tempo': tempo_val,\n",
    "                    'spectral_centroid': spectral_centroid,\n",
    "                    'zero_crossing_rate': zero_crossing_rate,\n",
    "                    'rmse': rmse,\n",
    "                    'bandwidth': bandwidth,\n",
    "                    'chroma': chroma,\n",
    "                    'beat_strength': beat_strength,\n",
    "                    'tempo_variation': tempo_variation,\n",
    "                    'onset_density': onset_density\n",
    "                }\n",
    "\n",
    "                # adaugam individual densitatile pentru fiecare valoare din mfcc\n",
    "                for i in range(13):\n",
    "                    feature[f\"mfcc{i+1}\"] = mfccs_mean[i]\n",
    "\n",
    "                features.append(feature)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[Eroare la {file}]: {e}\")\n",
    "\n",
    "# cream csv-ul care va fi utilizat de catre programul PySpark\n",
    "df = pd.DataFrame(features)\n",
    "df.to_csv('features/genre_features.csv', index=False)\n"
   ],
   "id": "d8d61f4d43ecc536",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Elena\\AppData\\Local\\Temp\\ipykernel_5936\\1837246080.py:17: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(path)\n",
      "C:\\Users\\Elena\\Documents\\big_data\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Eroare la jazz.00054.wav]: \n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Procesarea datelor",
   "id": "ab2b952b31087821"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In cadrul acestui pas voi initializa sesiunea de Spark, voi incarca datele extrase din fisierul .csv intr-un Dataframe si voi crea coloane noi pe baza datelor deja incarcate folosind agregari din SparkSQL.\n",
   "id": "a63c26bebb03010f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
    "from pyspark.sql.functions import expr, col, when, lit\n",
    "from pyspark.sql.functions import avg, stddev, count\n",
    "\n",
    "\n",
    "# IniÈ›ializare Spark\n",
    "spark = SparkSession.builder.appName(\"MusicClassification_CV\").getOrCreate()\n",
    "\n",
    "# Citirea si curatarea datelor\n",
    "df = spark.read.csv(\"features/genre_features.csv\", header=True, inferSchema=True).dropna()\n",
    "\n",
    "# Recreearea view-ului SQL care ne ajuta la crearea unor coloane suplimentare\n",
    "df.createOrReplaceTempView(\"audio_raw\")\n",
    "\n",
    "# Script SparkSQL pentru transformarea datelor\n",
    "sql_df = spark.sql(\"\"\"\n",
    "    SELECT *,\n",
    "           tempo * zero_crossing_rate AS rhythmic_complexity, -- complexitatea ritmica ajuta la recunoasterea melodiilor mai energice, cu variatii dese de ritm\n",
    "           chroma * spectral_centroid AS harmonic_density, -- densitatea armonica ajuta la masurarea inaltimii sunetelor facand diferenta dintre jazz si muzica clasica\n",
    "           CASE WHEN bandwidth != 0 THEN spectral_centroid / bandwidth ELSE 0.0 END AS brightness_score -- luminozitatea timbrala ajuta la masurarea concentrarii sunetului pe suprafata sonora\n",
    "    FROM audio_raw\n",
    "\"\"\")\n",
    "\n",
    "mfcc_cols = [f\"mfcc{i}\" for i in range(1, 14)]\n",
    "sql_df = sql_df.withColumn(\"mfccs_array\", expr(f\"array({', '.join(mfcc_cols)})\"))\n",
    "sql_df = sql_df.withColumn(\"mfcc_energy\", expr(\"aggregate(mfccs_array, 0D, (acc, x) -> acc + x) / size(mfccs_array)\")) # masoara consistenta timbrala\n",
    "sql_df = sql_df.withColumn(\"percussive_ratio\", when(\n",
    "    col(\"mfcc_energy\") != 0, col(\"rhythmic_complexity\") / col(\"mfcc_energy\")\n",
    ").otherwise(lit(0.0))) # indica dominanta ritmica in raport cu structura generala a piesei\n",
    "\n",
    "# renuntam la aceasta coloana intrucat nu este necesara in vederea antrenarii\n",
    "df_proc = sql_df.drop(\"mfccs_array\")\n",
    "\n",
    "# Calculam numarul, media, varianta si agregarea pe anumite coloane utilizate, grupate dupa genul muzical\n",
    "agg_df = df_proc.groupBy(\"genre\").agg(\n",
    "    count(\"*\").alias(\"num_tracks\"),\n",
    "    avg(\"tempo\").alias(\"avg_tempo\"),\n",
    "    avg(\"mfcc_energy\").alias(\"avg_mfcc_energy\"),\n",
    "    stddev(\"rhythmic_complexity\").alias(\"std_rhythmic_complexity\"),\n",
    "    avg(\"brightness_score\").alias(\"avg_brightness\"),\n",
    "    avg(\"harmonic_density\").alias(\"avg_harmonic_density\")\n",
    ")\n",
    "\n",
    "print(\"Afisarea rezultatelor agregate\")\n",
    "agg_df.show(truncate=False)\n",
    "\n",
    "# Transformam echitele genurilor in valori numerice\n",
    "indexer = StringIndexer(inputCol=\"genre\", outputCol=\"label\")\n",
    "df_indexed = indexer.fit(df_proc).transform(df_proc)\n",
    "\n",
    "label_to_genre = {i: genre for i, genre in enumerate(indexer.fit(df_proc).labels)}\n",
    "\n",
    "\n",
    "# Definim coloanele ce vor fi utilizate in procesul de antrenare\n",
    "feature_cols = [\n",
    "    \"tempo\", \"spectral_centroid\", \"zero_crossing_rate\", \"rmse\", \"bandwidth\", \"chroma\",\n",
    "    \"mfcc_energy\", \"rhythmic_complexity\", \"harmonic_density\", \"brightness_score\", \"percussive_ratio\",\n",
    "    \"mfcc1\", \"mfcc2\", \"mfcc3\", \"mfcc4\", \"mfcc5\", \"mfcc6\", \"mfcc7\", \"mfcc8\", \"mfcc9\", \"mfcc10\", \"mfcc11\", \"mfcc12\", \"mfcc13\",\n",
    "    \"beat_strength\", \"tempo_variation\", \"onset_density\"\n",
    "]"
   ],
   "id": "bff40c49b699c33a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Output-ul sectiunii de agregare:",
   "id": "cb498d56980d811e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<pre>\n",
    "+---------+----------+------------------+-------------------+-----------------------+------------------+--------------------+\n",
    "|genre    |num_tracks|avg_tempo         |avg_mfcc_energy    |std_rhythmic_complexity|avg_brightness    |avg_harmonic_density|\n",
    "+---------+----------+------------------+-------------------+-----------------------+------------------+--------------------+\n",
    "|pop      |100       |113.85883803911639|4.3668675117292315 |4.296904390620976      |1.0145147030871682|1239.1377810527329  |\n",
    "|blues    |100       |123.87949190718429|-0.6201062277244623|4.685624017080769      |0.8798321935538955|601.6419940667206   |\n",
    "|hiphop   |100       |110.20885930181859|3.773816669750768  |4.574478967875037      |1.0030175492772835|1151.0834937279783  |\n",
    "|jazz     |99        |119.37343977271567|-5.294334864837605 |4.936284132646595      |0.8725799141399381|553.3636452407295   |\n",
    "|country  |100       |120.65124481408883|0.14414083058038496|4.4447313663647385     |0.8900325176010897|667.2254004402726   |\n",
    "|metal    |100       |123.84717744375791|5.191095756484617  |4.517912628998164      |1.1584910678154894|1254.6358880884725  |\n",
    "|rock     |100       |120.19167310740562|1.5113764001299996 |4.7895477160549        |0.9873583989421267|871.7691659381758   |\n",
    "|reggae   |100       |132.7436194595435 |0.2918145789961539 |5.575508307501024      |0.9353338996148348|905.570612814782    |\n",
    "|disco    |100       |120.00554444511342|2.1085045316690763 |3.9301408372302946     |1.0397397660041572|1095.177341635532   |\n",
    "|classical|100       |125.7456965589809 |-15.178653889158692|3.4972039388406935     |0.8837323296755996|360.0458398382722   |\n",
    "+---------+----------+------------------+-------------------+-----------------------+------------------+--------------------+\n",
    "</pre>"
   ],
   "id": "16a892baaca74d38"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Aplicarea metodelor ML",
   "id": "583719632cffe827"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### a. Random Forest Classifier",
   "id": "5cf28b7f9c41c91e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Am ales utilizarea acestui model de Machine Learning intrucat este eficient in problemele de clasificare. Acesta foloseste un numar variat de arbori de decizie care favorizeaza un rezultat apropiat de realitate in momentul antrenarii pe mai multe date ce reprezinta caracteristici interdepedente, asa cum sunt cele legate de sunetele dintr-o creatie muzicala. In plus, gestioneaza eficient cantitati mari de date. Folosind acest model am evaluat acuratetea predictiei unei melodii la oricare dintre genurile muzicale prezente in setul de antrenament. O trasatura importanta a acestui model este aceea de \"feature importance\", care modeleaza antrenarea in functie de cele mai importante caracteristici.",
   "id": "b4ddc63d4434e8d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyspark.ml.feature import  VectorAssembler, StandardScaler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "def print_confusion_matrix(predictions, model_name, label_to_genre):\n",
    "    pred_labels = predictions.select(\"label\", \"prediction\")\n",
    "    confusion_matrix = pred_labels.groupBy(\"label\", \"prediction\").count().orderBy(\"label\", \"prediction\")\n",
    "\n",
    "    labels = sorted(label_to_genre.keys())\n",
    "    confusion = confusion_matrix.collect()\n",
    "    conf_dict = {(row['label'], row['prediction']): row['count'] for row in confusion}\n",
    "\n",
    "    print(f\"\\nMatrice de confuzie pentru {model_name}\")\n",
    "    print(\" \" * 18 + \"\\t\".join([label_to_genre[l] for l in labels]))\n",
    "    for actual in labels:\n",
    "        row_counts = [str(conf_dict.get((actual, pred), 0)) for pred in labels]\n",
    "        print(f\"{label_to_genre[actual]:18}:\\t\" + \"\\t\".join(row_counts))\n",
    "\n",
    "\n",
    "# Definim coloanele ce vor fi utilizate in procesul de antrenare\n",
    "feature_cols = [\n",
    "    \"tempo\", \"spectral_centroid\", \"zero_crossing_rate\", \"rmse\", \"bandwidth\", \"chroma\",\n",
    "    \"mfcc_energy\", \"rhythmic_complexity\", \"harmonic_density\", \"brightness_score\", \"percussive_ratio\",\n",
    "    \"mfcc1\", \"mfcc2\", \"mfcc3\", \"mfcc4\", \"mfcc5\", \"mfcc6\", \"mfcc7\", \"mfcc8\", \"mfcc9\", \"mfcc10\", \"mfcc11\", \"mfcc12\", \"mfcc13\",\n",
    "    \"beat_strength\", \"tempo_variation\", \"onset_density\"\n",
    "]\n",
    "\n",
    "# Impachetam datele pentru a putea fi folosite mai departe in procesul de antrenare\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_vec\")\n",
    "# Normalizam datele\n",
    "scaler = StandardScaler(inputCol=\"features_vec\", outputCol=\"features\", withMean=True, withStd=True)\n",
    "\n",
    "# Impartim datele intre cele de antrenare si de testare\n",
    "train_data, test_data = df_indexed.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "# Initializarea modelului\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
    "# Cream un pipeline pentru a automatiza tot procesul\n",
    "rf_pipeline = Pipeline(stages=[assembler, scaler, rf])\n",
    "\n",
    "# Construim grila de combinatii de hiperparametrii\n",
    "rf_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf_pipeline.getStages()[2].numTrees, [100, 150]) \\\n",
    "    .addGrid(rf_pipeline.getStages()[2].maxDepth, [5, 10]) \\\n",
    "    .addGrid(rf_pipeline.getStages()[2].maxBins, [32]) \\\n",
    "    .addGrid(rf_pipeline.getStages()[2].featureSubsetStrategy, ['auto', 'sqrt']) \\\n",
    "    .build()\n",
    "\n",
    "# Configuram validarea incrucisata\n",
    "rf_cv = CrossValidator(\n",
    "    estimator=rf_pipeline,\n",
    "    estimatorParamMaps=rf_paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=5\n",
    ")\n",
    "\n",
    "# Antrenam si evaluam modelul\n",
    "rf_cv_model = rf_cv.fit(train_data)\n",
    "rf_preds = rf_cv_model.transform(test_data)\n",
    "rf_acc = evaluator.evaluate(rf_preds)\n",
    "\n",
    "print(f\" Random Forest Accuracy:       {rf_acc:.3f}\")\n",
    "print_confusion_matrix(rf_preds, \"Random Forest\", label_to_genre)\n"
   ],
   "id": "e0e5da7c03109a6b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Output acuratete RF:",
   "id": "5316c5c8e2eba0c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Random Forest Accuracy:       0.770",
   "id": "5d1699c139aed864"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Output matrice de confuzie RF:",
   "id": "4bd3308c75daf4c7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<pre>\n",
    "                  blues\tclassical\tcountry\tdisco\thiphop\tmetal\tpop\treggae\trock\tjazz\n",
    "blues             :\t16\t0\t0\t0\t0\t0\t0\t0\t1\t0\n",
    "classical         :\t0\t12\t1\t0\t0\t0\t0\t0\t1\t0\n",
    "country           :\t2\t0\t12\t0\t0\t0\t0\t1\t0\t1\n",
    "disco             :\t0\t0\t1\t6\t2\t0\t1\t0\t1\t0\n",
    "hiphop            :\t0\t0\t0\t0\t14\t0\t2\t0\t0\t0\n",
    "metal             :\t0\t0\t0\t0\t0\t17\t0\t0\t0\t0\n",
    "pop               :\t0\t1\t1\t0\t0\t0\t12\t1\t0\t2\n",
    "reggae            :\t1\t1\t1\t2\t1\t0\t0\t10\t0\t1\n",
    "rock              :\t1\t0\t2\t1\t1\t2\t0\t0\t8\t0\n",
    "jazz              :\t0\t1\t2\t0\t0\t0\t0\t0\t1\t17\n",
    "</pre>"
   ],
   "id": "e8f0703a2bf21a4a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### b. Gradient Boosted Tree Clasifier",
   "id": "36021f14773a5f9d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "De asemenea, o optiune buna in problemele de clasificare, GBT isi construieste arborii de decizie secvential, invatand continuu de la predecesori, fiind o solutie eficienta in ceea ce priveste clasificarea genurilor muzicale care prezinta particularitati asemenatoare si care pot fi cu usurinta confundate de catre modelul RF. GBT accepta doar clase binare, prin urmare, folosind acest model, am ales sa evaluez capacitatea modelului de a distinge o melodie incadrata intr-o anumita categorie.",
   "id": "584b4a4df748d8fe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Conform matricei de confuzie furnizate de modelul RF se poate observa cum categoria \"disco\" e cel mai slab clasificata, prin urmare, prin modelul GBT urmarim sa vedem performanta modelului de a clasifica o melodie apartinand genului \"disco\", fata de oricare alta.",
   "id": "161808220f50e5a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Cream array-ul de etichete\n",
    "df_binary = df_indexed.withColumn(\"label_disco\", when(col(\"genre\") == \"disco\", 1).otherwise(0))\n",
    "\n",
    "gtb_label_to_genre = {0: \"not_disco\", 1: \"disco\"}\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_vec\")\n",
    "scaler = StandardScaler(inputCol=\"features_vec\", outputCol=\"features\", withMean=True, withStd=True)\n",
    "\n",
    "gbt = GBTClassifier(featuresCol=\"features\", labelCol=\"label_disco\", maxIter=100)\n",
    "gbt_pipeline = Pipeline(stages=[assembler, scaler, gbt])\n",
    "\n",
    "gbt_train_data, gbt_test_data = df_binary.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "gbt_paramGrid = ParamGridBuilder() \\\n",
    "                .addGrid(gbt.maxDepth, [2]) \\\n",
    "                .addGrid(gbt.maxIter, [50]) \\\n",
    "                .build()\n",
    "\n",
    "gb_evaluator = MulticlassClassificationEvaluator(labelCol=\"label_disco\")\n",
    "\n",
    "gbt_cv = CrossValidator(\n",
    "    estimator=gbt_pipeline,\n",
    "    estimatorParamMaps=gbt_paramGrid,\n",
    "    evaluator=gb_evaluator,\n",
    "    numFolds=5\n",
    ")\n",
    "\n",
    "gbt_cv_model = gbt_cv.fit(gbt_train_data)\n",
    "gbt_predictions = gbt_cv_model.transform(gbt_test_data)\n",
    "gbt_acc = gb_evaluator.evaluate(gbt_predictions)\n",
    "\n",
    "print(f\" GBTClassifier  Accuracy: {gbt_acc:.3f}\")"
   ],
   "id": "13522b0f917f0ce8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Output acuratete GBT:",
   "id": "87526e6a4f1a3fe4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "GBTClassifier  Accuracy: 0.913",
   "id": "b390e883057e851e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Data Pipeline",
   "id": "9607be0e5e4410c7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Am utilizat cate un pipeline pentru fiecare dintre cele doua modele de ML, astfel:",
   "id": "16f09264bd6fc860"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Pipeline RF\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_vec\")\n",
    "scaler = StandardScaler(inputCol=\"features_vec\", outputCol=\"features\", withMean=True, withStd=True)\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
    "rf_pipeline = Pipeline(stages=[assembler, scaler, rf])\n",
    "\n",
    "# Pipeline GBT\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_vec\")\n",
    "scaler = StandardScaler(inputCol=\"features_vec\", outputCol=\"features\", withMean=True, withStd=True)\n",
    "\n",
    "gbt = GBTClassifier(featuresCol=\"features\", labelCol=\"label_disco\", maxIter=100)\n",
    "gbt_pipeline = Pipeline(stages=[assembler, scaler, gbt])"
   ],
   "id": "bf4e018fe0084f0a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Optimizarea hiperparametrilor",
   "id": "70b52e6761bce9a6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Am folosit aceasta tehnica pentru fiecare dintre cele doua modele, astfel:",
   "id": "ae80b1c66921975a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# RF\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
    "rf_pipeline = Pipeline(stages=[assembler, scaler, rf])\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "rf_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf_pipeline.getStages()[2].numTrees, [100, 150]) \\\n",
    "    .addGrid(rf_pipeline.getStages()[2].maxDepth, [5, 10]) \\\n",
    "    .addGrid(rf_pipeline.getStages()[2].maxBins, [32]) \\\n",
    "    .addGrid(rf_pipeline.getStages()[2].featureSubsetStrategy, ['auto', 'sqrt']) \\\n",
    "    .build()\n",
    "\n",
    "rf_cv = CrossValidator(\n",
    "    estimator=rf_pipeline,\n",
    "    estimatorParamMaps=rf_paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=5\n",
    ")\n",
    "\n",
    "# GBT\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_vec\")\n",
    "scaler = StandardScaler(inputCol=\"features_vec\", outputCol=\"features\", withMean=True, withStd=True)\n",
    "\n",
    "gbt = GBTClassifier(featuresCol=\"features\", labelCol=\"label_disco\", maxIter=100)\n",
    "gbt_pipeline = Pipeline(stages=[assembler, scaler, gbt])\n",
    "\n",
    "gbt_train_data, gbt_test_data = df_binary.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "gbt_paramGrid = ParamGridBuilder() \\\n",
    "                .addGrid(gbt.maxDepth, [2]) \\\n",
    "                .addGrid(gbt.maxIter, [50]) \\\n",
    "                .build()\n",
    "\n",
    "gb_evaluator = MulticlassClassificationEvaluator(labelCol=\"label_disco\")\n",
    "\n",
    "gbt_cv = CrossValidator(\n",
    "    estimator=gbt_pipeline,\n",
    "    estimatorParamMaps=gbt_paramGrid,\n",
    "    evaluator=gb_evaluator,\n",
    "    numFolds=5\n",
    ")\n"
   ],
   "id": "f9c7f715c19f222"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. Aplicarea unei metode DL",
   "id": "6cb0390549b860c0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Pentru a rezolva aceasta cerinta am ales utilizarea metodei Keras Sequential, un model de retea neuronala, performanta pe problemele de clasificare si care are o capacitate buna de gestiune a datelor non-liniare, cu o relatie complexa intre caracteristici.",
   "id": "a83aea015ce86706"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In primul rand, am exportat datele importante deja procesate de catre modelele anterioare, creand noi .csv-uri folosite ca si input-uri pentru noul model KS.",
   "id": "f62e1b1ad0601c5c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "\n",
    "# Aleg modelul RF optim\n",
    "final_df = rf_cv_model.bestModel.transform(df_indexed)\n",
    "# Preiau datele necesare antrenarii\n",
    "pandas_df = final_df.select(\"features\", \"label\").toPandas()\n",
    "\n",
    "X = np.array(pandas_df[\"features\"].tolist())\n",
    "y = pandas_df[\"label\"].values\n",
    "\n",
    "# Salvez in CSV\n",
    "pd.DataFrame(X).to_csv(\"features.csv\", index=False)\n",
    "pd.DataFrame(y, columns=[\"label\"]).to_csv(\"labels.csv\", index=False)"
   ],
   "id": "cdcbac36083fab0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Urmand ca aceste date sa fie folosite mai departe in noul model KS.",
   "id": "66c6bf134046d2d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "X = pd.read_csv('/kaggle/input/dataset2/features.csv').values\n",
    "y = pd.read_csv('/kaggle/input/dataset2/labels.csv').values.ravel()\n",
    "\n",
    "# Pre-procesarea array-ului de etichete\n",
    "y_cat = to_categorical(y)\n",
    "\n",
    "# Impartirea datelor in date de test si de antrenament\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_cat, test_size=0.2, random_state=42)\n",
    "\n",
    "# Definirea modelului\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.BatchNormalization(input_shape=(X.shape[1],)),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(y_cat.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "# Pentru compilarea modelului am adaugat optimizatorul clasic \"adam\" si functia de pierdere \"categorical_crossentropy\", standard in problemele de clasificare cu mai multe clase\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)\n",
    "\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {acc:.3f}\")"
   ],
   "id": "3025026c9731be17"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Output KS:",
   "id": "d6dd8fccf9c321a9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Test Accuracy: 0.765",
   "id": "33882bd6cd04752"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Concluzii",
   "id": "3326b7d2e5d02c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In urma acestor rezultate observam ca RF si KS au performante similare, in timp ce GBT captureaza eficient diferentele subtile ale categoriei slab clasificata de catre RF, reusind sa intoarca o acuratete notabila.",
   "id": "510e265f3b207c02"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Cuprins\n",
    "1. [Introducere - prezentarea setului de date si enuntarea obiectivelor](#1-introducere---prezentarea-setului-de-date-si-enuntarea-obiectivelor)\n",
    "   - [Preprocesarea datelor](#preprocesarea-datelor)\n",
    "2. [Procesarea datelor](#2-procesarea-datelor)\n",
    "3. [Aplicarea metodelor ML](#3-aplicarea-metodelor-ml)\n",
    "   - [Random Forest Classifier](#a-random-forest-classifier)\n",
    "   - [Gradient Boosted Tree Classifier](#b-gradient-boosted-tree-clasifier)\n",
    "4. [Data Pipeline](#4-data-pipeline)\n",
    "5. [Optimizarea hiperparametrilor](#5-optimizarea-hiperparametrilor)\n",
    "6. [Aplicarea unei metode DL](#6-aplicarea-unei-metode-dl)\n",
    "7. [Concluzii](#concluzii)\n"
   ],
   "id": "95b96103ffbd6472"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Sursa preprocesare",
   "id": "162eaabdb04aef60"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import librosa\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from librosa.feature.rhythm import tempo as rhythm_tempo\n",
    "\n",
    "base_path = \"data/Data/genres_original\"\n",
    "genres = os.listdir(base_path)\n",
    "features = []\n",
    "\n",
    "for genre in genres:\n",
    "    genre_path = os.path.join(base_path, genre)\n",
    "    for file in os.listdir(genre_path):\n",
    "        if file.endswith(\".wav\"):\n",
    "            path = os.path.join(genre_path, file)\n",
    "            try:\n",
    "                y, sr = librosa.load(path)\n",
    "                tempo_val = rhythm_tempo(y=y, sr=sr)[0] # variatia de ritm a piesei\n",
    "                spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr).mean() # inaltimea sunetului\n",
    "                zero_crossing_rate = librosa.feature.zero_crossing_rate(y).mean() # frecventa valorii 0 in evolutia semnalului - ajuta la identificarea zgomotelor percutante\n",
    "                rmse = librosa.feature.rms(y=y).mean() # Root Mean Square Energy - energia generala a semnalului\n",
    "                bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr).mean() # latimea benzii de frecventa - urmareste cat de mult variaza semnalul\n",
    "                chroma = librosa.feature.chroma_stft(y=y, sr=sr).mean() # distributia semnalului pe notele muzicale - reflecta tonalitatea\n",
    "                mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13) # Mel-Frequency Cepstral Coefficients - reprezentare compacta a sunetului, apropiata de perceptia umana\n",
    "                mfccs_mean = mfccs.mean(axis=1) # calculam media valorilor mfcc extrase\n",
    "                beat_strength = np.mean(librosa.onset.onset_strength(y=y, sr=sr)) # puterea batailor\n",
    "                tempogram = librosa.feature.tempogram(y=y, sr=sr)\n",
    "                tempo_variation = np.std(tempogram) # deviatia tempo-ului\n",
    "                onsets = librosa.onset.onset_detect(y=y, sr=sr)\n",
    "                onset_density = len(onsets) / librosa.get_duration(y=y, sr=sr) # densitatea evenimentelor sonore\n",
    "\n",
    "                feature = {\n",
    "                    'filename': file,\n",
    "                    'genre': genre,\n",
    "                    'tempo': tempo_val,\n",
    "                    'spectral_centroid': spectral_centroid,\n",
    "                    'zero_crossing_rate': zero_crossing_rate,\n",
    "                    'rmse': rmse,\n",
    "                    'bandwidth': bandwidth,\n",
    "                    'chroma': chroma,\n",
    "                    'beat_strength': beat_strength,\n",
    "                    'tempo_variation': tempo_variation,\n",
    "                    'onset_density': onset_density\n",
    "                }\n",
    "\n",
    "                # adaugam individual densitatile pentru fiecare valoare din mfcc\n",
    "                for i in range(13):\n",
    "                    feature[f\"mfcc{i+1}\"] = mfccs_mean[i]\n",
    "\n",
    "                features.append(feature)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[Eroare la {file}]: {e}\")\n",
    "\n",
    "# cream csv-ul care va fi utilizat de catre programul PySpark\n",
    "df = pd.DataFrame(features)\n",
    "df.to_csv('features/genre_features.csv', index=False)\n"
   ],
   "id": "6227ed7c728dbead"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Sursa cerinte 2-",
   "id": "ced8dd12cfe477a9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression, GBTClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql.functions import expr, size, col, when, lit\n",
    "from pyspark.sql.functions import avg, stddev, count\n",
    "\n",
    "\n",
    "def print_confusion_matrix(predictions, model_name, label_to_genre):\n",
    "    pred_labels = predictions.select(\"label\", \"prediction\")\n",
    "    confusion_matrix = pred_labels.groupBy(\"label\", \"prediction\").count().orderBy(\"label\", \"prediction\")\n",
    "\n",
    "    labels = sorted(label_to_genre.keys())\n",
    "    confusion = confusion_matrix.collect()\n",
    "    conf_dict = {(row['label'], row['prediction']): row['count'] for row in confusion}\n",
    "\n",
    "    print(f\"\\nMatrice de confuzie pentru {model_name}\")\n",
    "    print(\" \" * 18 + \"\\t\".join([label_to_genre[l] for l in labels]))\n",
    "    for actual in labels:\n",
    "        row_counts = [str(conf_dict.get((actual, pred), 0)) for pred in labels]\n",
    "        print(f\"{label_to_genre[actual]:18}:\\t\" + \"\\t\".join(row_counts))\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MusicClassification_CV\").getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"features/genre_features.csv\", header=True, inferSchema=True).dropna()\n",
    "df.createOrReplaceTempView(\"audio_raw\")\n",
    "\n",
    "sql_df = spark.sql(\"\"\"\n",
    "    SELECT *,\n",
    "           tempo * zero_crossing_rate AS rhythmic_complexity,\n",
    "           chroma * spectral_centroid AS harmonic_density,\n",
    "           CASE WHEN bandwidth != 0 THEN spectral_centroid / bandwidth ELSE 0.0 END AS brightness_score\n",
    "    FROM audio_raw\n",
    "\"\"\")\n",
    "\n",
    "mfcc_cols = [f\"mfcc{i}\" for i in range(1, 14)]\n",
    "sql_df = sql_df.withColumn(\"mfccs_array\", expr(f\"array({', '.join(mfcc_cols)})\"))\n",
    "sql_df = sql_df.withColumn(\"mfcc_energy\", expr(\"aggregate(mfccs_array, 0D, (acc, x) -> acc + x) / size(mfccs_array)\"))\n",
    "sql_df = sql_df.withColumn(\"percussive_ratio\", when(\n",
    "    col(\"mfcc_energy\") != 0, col(\"rhythmic_complexity\") / col(\"mfcc_energy\")\n",
    ").otherwise(lit(0.0)))\n",
    "\n",
    "df_proc = sql_df.drop(\"mfccs_array\")\n",
    "\n",
    "agg_df = df_proc.groupBy(\"genre\").agg(\n",
    "    count(\"*\").alias(\"num_tracks\"),\n",
    "    avg(\"tempo\").alias(\"avg_tempo\"),\n",
    "    avg(\"mfcc_energy\").alias(\"avg_mfcc_energy\"),\n",
    "    stddev(\"rhythmic_complexity\").alias(\"std_rhythmic_complexity\"),\n",
    "    avg(\"brightness_score\").alias(\"avg_brightness\"),\n",
    "    avg(\"harmonic_density\").alias(\"avg_harmonic_density\")\n",
    ")\n",
    "\n",
    "print(\"Afisarea rezultatelor agregate\")\n",
    "agg_df.show(truncate=False)\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"genre\", outputCol=\"label\")\n",
    "df_indexed = indexer.fit(df_proc).transform(df_proc)\n",
    "\n",
    "label_to_genre = {i: genre for i, genre in enumerate(indexer.fit(df_proc).labels)}\n",
    "\n",
    "feature_cols = [\n",
    "    \"tempo\", \"spectral_centroid\", \"zero_crossing_rate\", \"rmse\", \"bandwidth\", \"chroma\",\n",
    "    \"mfcc_energy\", \"rhythmic_complexity\", \"harmonic_density\", \"brightness_score\", \"percussive_ratio\",\n",
    "    \"mfcc1\", \"mfcc2\", \"mfcc3\", \"mfcc4\", \"mfcc5\", \"mfcc6\", \"mfcc7\", \"mfcc8\", \"mfcc9\", \"mfcc10\", \"mfcc11\", \"mfcc12\", \"mfcc13\",\n",
    "    \"beat_strength\", \"tempo_variation\", \"onset_density\"\n",
    "]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_vec\")\n",
    "scaler = StandardScaler(inputCol=\"features_vec\", outputCol=\"features\", withMean=True, withStd=True)\n",
    "\n",
    "train_data, test_data = df_indexed.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
    "rf_pipeline = Pipeline(stages=[assembler, scaler, rf])\n",
    "\n",
    "rf_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf_pipeline.getStages()[2].numTrees, [100, 150]) \\\n",
    "    .addGrid(rf_pipeline.getStages()[2].maxDepth, [5, 10]) \\\n",
    "    .addGrid(rf_pipeline.getStages()[2].maxBins, [32]) \\\n",
    "    .addGrid(rf_pipeline.getStages()[2].featureSubsetStrategy, ['auto', 'sqrt']) \\\n",
    "    .build()\n",
    "\n",
    "rf_cv = CrossValidator(\n",
    "    estimator=rf_pipeline,\n",
    "    estimatorParamMaps=rf_paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=5\n",
    ")\n",
    "\n",
    "rf_cv_model = rf_cv.fit(train_data)\n",
    "rf_preds = rf_cv_model.transform(test_data)\n",
    "rf_acc = evaluator.evaluate(rf_preds)\n",
    "\n",
    "df_binary = df_indexed.withColumn(\"label_disco\", when(col(\"genre\") == \"disco\", 1).otherwise(0))\n",
    "\n",
    "gtb_label_to_genre = {0: \"not_disco\", 1: \"disco\"}\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_vec\")\n",
    "scaler = StandardScaler(inputCol=\"features_vec\", outputCol=\"features\", withMean=True, withStd=True)\n",
    "\n",
    "gbt = GBTClassifier(featuresCol=\"features\", labelCol=\"label_disco\", maxIter=100)\n",
    "gbt_pipeline = Pipeline(stages=[assembler, scaler, gbt])\n",
    "\n",
    "gbt_train_data, gbt_test_data = df_binary.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "gbt_paramGrid = ParamGridBuilder() \\\n",
    "                .addGrid(gbt.maxDepth, [2]) \\\n",
    "                .addGrid(gbt.maxIter, [50]) \\\n",
    "                .build()\n",
    "\n",
    "gb_evaluator = MulticlassClassificationEvaluator(labelCol=\"label_disco\")\n",
    "\n",
    "gbt_cv = CrossValidator(\n",
    "    estimator=gbt_pipeline,\n",
    "    estimatorParamMaps=gbt_paramGrid,\n",
    "    evaluator=gb_evaluator,\n",
    "    numFolds=5\n",
    ")\n",
    "\n",
    "gbt_cv_model = gbt_cv.fit(gbt_train_data)\n",
    "gbt_predictions = gbt_cv_model.transform(gbt_test_data)\n",
    "gbt_acc = gb_evaluator.evaluate(gbt_predictions)\n",
    "\n",
    "print(f\" Random Forest Accuracy (CV):       {rf_acc:.3f}\")\n",
    "print(f\" GBTClassifier  Accuracy (CV): {gbt_acc:.3f}\")\n",
    "\n",
    "print(\" Matrice confuzie RF:\")\n",
    "print_confusion_matrix(rf_preds, \"Random Forest\", label_to_genre)\n",
    "\n",
    "# print(\"Matrice confuzie GBT:\")\n",
    "# print_confusion_matrix(gbt_predictions, \"GBTClassifier\", gtb_label_to_genre)\n",
    "\n",
    "final_df = rf_cv_model.bestModel.transform(df_indexed)\n",
    "pandas_df = final_df.select(\"features\", \"label\").toPandas()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X = np.array(pandas_df[\"features\"].tolist())  # features = list de vectori\n",
    "y = pandas_df[\"label\"].values\n",
    "\n",
    "pd.DataFrame(X).to_csv(\"features.csv\", index=False)\n",
    "pd.DataFrame(y, columns=[\"label\"]).to_csv(\"labels.csv\", index=False)\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = pd.read_csv('/kaggle/input/dataset2/features.csv').values\n",
    "y = pd.read_csv('/kaggle/input/dataset2/labels.csv').values.ravel()\n",
    "\n",
    "y_cat = to_categorical(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_cat, test_size=0.2, random_state=42)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.BatchNormalization(input_shape=(X.shape[1],)),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(y_cat.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)\n",
    "\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {acc:.3f}\")\n",
    "\n",
    "\n"
   ],
   "id": "bea4574ab2d9b7ff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Output 2-5",
   "id": "fd8efba24cf1cf41"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "d8ec030cce9f4a0c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
